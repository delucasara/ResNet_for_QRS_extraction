{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResNet1d.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK4YvBQkecw6",
        "outputId": "ee27efe6-c980-46fc-8860-7a312d3bfc90"
      },
      "source": [
        "!pip install torchsummary"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37sVaz4qdTaT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # self.bn1 = nn.GroupNorm(planes//2, planes)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # self.bn2 = nn.GroupNorm(planes//2, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                    nn.Conv1d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm1d(self.expansion*planes)\n",
        "                    )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        # self.bn1 = nn.GroupNorm(planes//2, planes)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # self.bn2 = nn.GroupNorm(planes//2, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        # self.bn2 = nn.GroupNorm(self.expansion*planes//2, self.expansion*planes)\n",
        "        self.bn3 = nn.BatchNorm1d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                    nn.Conv1d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm1d(self.expansion*planes)\n",
        "                    )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 128\n",
        "        \n",
        "        # self.avg1 = nn.AdaptiveAvgPool1d(5000)\n",
        "        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 128, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 256, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 512, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 1024, num_blocks[3], stride=2)\n",
        "        # self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.linear1 = nn.Linear(9216*block.expansion, 1024)\n",
        "        self.linear2 = nn.Linear(1024, num_classes)\n",
        "\n",
        "    \n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # out = self.avg1(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        " \n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        #out = F.avg_pool1d(out, 4)\n",
        "        out = F.avg_pool1d(out, 16)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear1(out)\n",
        "        out = self.linear2(out)\n",
        "        # out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# These architectures are for Nanopore Translocation Signal Features Prediction\n",
        "# In our case we predict the number of translocation events inside a window in a trace\n",
        "def ResNet10_Counter():\n",
        "    return ResNet(BasicBlock, [1, 1, 1, 1], num_classes=1)\n",
        "\n",
        "def ResNet18_Counter():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=1)\n",
        "\n",
        "def ResNet34_Counter():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=1)\n",
        "\n",
        "def ResNet50_Counter():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=1)\n",
        "\n",
        "def ResNet101_Counter():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=1)\n",
        "\n",
        "def ResNet152_Counter():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes=1)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0U4tVqgL04"
      },
      "source": [
        "from numpy import vstack\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.optim import SGD, Adam, Adagrad\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import time\n",
        " \n",
        "# dataset definition\n",
        "class CSVDataset(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, path, normalize = False):\n",
        "        global min_y\n",
        "        global max_y\n",
        "        # load the csv file as a dataframe\n",
        "        df = read_csv(path)\n",
        "        df = df.transpose()\n",
        "        # store the inputs and outputs\n",
        "        self.X = df.values[:100, :-1]\n",
        "        # print(self.X)\n",
        "        self.y = df.values[:100, -1]      \n",
        "        #print(\"Dataset length: \", self.X.shape[0])\n",
        "        \n",
        "        # ensure input data is floats\n",
        "        self.X = self.X.astype(np.float)\n",
        "        self.y = self.y.astype(np.float)\n",
        "        # print(self.y)\n",
        "\n",
        "        if normalize:\n",
        "            self.X = self.X.reshape(self.X.shape[1], self.X.shape[0])\n",
        "            min_X = np.min(self.X,0)  # returns an array of means for each beat\n",
        "            max_X = np.max(self.X,0)\n",
        "            self.X = (self.X - min_X)/(max_X-min_X)\n",
        "            min_y = np.min(self.y)\n",
        "            max_y = np.max(self.y)\n",
        "            self.y = (self.y - min_y)/(max_y-min_y)\n",
        "            \n",
        "            # #UNCOMMENT IN CASE OF OVERFITTING 1 SAMPLE\n",
        "            # min_X = np.min(self.X)  \n",
        "            # max_X = np.max(self.X)\n",
        "            # self.X = (self.X - min_X)/(max_X-min_X)\n",
        "            # print(self.X)\n",
        "            # self.X = self.X.reshape(1, 1, 1200)\n",
        "\n",
        "            \n",
        "        # reshape input data\n",
        "        self.X = self.X.reshape(self.X.shape[1], 1, self.X.shape[0])\n",
        "        self.y = self.y.reshape(self.y.shape[0],)\n",
        "        # label encode target and ensure the values are floats\n",
        "        # self.y = LabelEncoder().fit_transform(self.y)\n",
        "        self.y = self.y.astype(np.float)\n",
        " \n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        " \n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        " \n",
        "    # get indexes for train and test rows\n",
        "    def get_splits(self, n_valid=0.2,n_test=0.2):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        valid_size = round(n_valid * len(self.X))\n",
        "        train_size = len(self.X) - test_size - valid_size\n",
        "        \n",
        "        train_set, val_set, test_set = random_split(self, [train_size, valid_size, test_size])\n",
        "        # calculate the split\n",
        "        return train_set, val_set, test_set\n",
        "    \n",
        " \n",
        " \n",
        "# prepare the dataset\n",
        "def prepare_data(path,batch_size):\n",
        "    # load the dataset\n",
        "    dataset = CSVDataset(path, normalize = True)\n",
        "    # calculate split\n",
        "    train, validation, test = dataset.get_splits()\n",
        "    \n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "    valid_dl = DataLoader(validation, batch_size=batch_size, shuffle=False)\n",
        "    test_dl = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        " \n",
        "    return train_dl, valid_dl, test_dl\n",
        "\n",
        " \n",
        "# train the model\n",
        "def train_model(train_dl, valid_dl, model, learning_rate, momentum, optimizer, model_name):\n",
        "    global start\n",
        "\n",
        "    PATH = \"./drive/MyDrive/Tesi/model.pt\"\n",
        "    # define the optimization\n",
        "    # criterion = BCEWithLogitsLoss()\n",
        "    # criterion = nn.MSELoss()\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    if optimizer == \"adam\":\n",
        "        optimizer = Adam(model.parameters(), lr=learning_rate, eps=momentum)\n",
        "    elif optimizer == \"sgd\":\n",
        "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    elif optimizer == \"adagrad\":\n",
        "        optimizer = Adagrad(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model = model.float()\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "      criterion.cuda()\n",
        "    loss_history = list()\n",
        "    valid_loss_history = list()\n",
        "    # enumerate epochs\n",
        "    start = time.time()\n",
        "\n",
        "    # checkpoint = torch.load(PATH)\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    # epoch = checkpoint['epoch']\n",
        "    # loss = checkpoint['loss']\n",
        "\n",
        "    for epoch in range(200):\n",
        "        # s = \"Epoch \" + str(epoch+1) + \"/5\"\n",
        "        # print(s, end=\"\\t\")\n",
        "        train_loss = 0.0\n",
        "        # enumerate mini batches\n",
        "        for i, (inputs, targets) in enumerate(iter(train_dl)):\n",
        "            targets = torch.reshape(targets, (targets.shape[0], 1))\n",
        "            # Transfer Data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                # print(\"GPU found\")\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat = model(inputs.float())\n",
        "            # print(\"Prediction vs Target\", round(yhat.item(),1), targets.item(),1, end = \"\\t\")\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, targets.float())\n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "            train_loss += loss\n",
        "            \n",
        "        loss_history.append(train_loss.item())\n",
        "        # print(\"train loss: \"+str(train_loss.item()),end=\"\\t\")\n",
        "        \n",
        "        #VALIDATE NETWORK\n",
        "        valid_loss = 0.0\n",
        "        #model.eval()     # Optional when not using Model Specific layer\n",
        "        for i, (inputs, targets) in enumerate(iter(valid_dl)):\n",
        "            targets = torch.reshape(targets, (targets.shape[0], 1))\n",
        "            # # Transfer Data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            # Forward Pass\n",
        "            yhat = model(inputs.float())\n",
        "            # Find the Loss\n",
        "            loss = criterion(yhat,targets.float())\n",
        "            # Calculate Loss\n",
        "            valid_loss += loss\n",
        "            \n",
        "        valid_loss_history.append(valid_loss.item())\n",
        "        # print(\"validation loss: \"+str(valid_loss.item()),end=\"\\n\")\n",
        "        \n",
        "        # torch.save({\n",
        "        #             'epoch': epoch,\n",
        "        #             'model_state_dict': model.state_dict(),\n",
        "        #             'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #             'train_loss': loss_history,\n",
        "        #             'valid_loss': valid_loss_history,\n",
        "        #             }, PATH)\n",
        "            \n",
        "    # plt.figure()\n",
        "    # plt.title(\"Loss history - \"+model_name)\n",
        "    # plt.xlabel(\"Epochs\")\n",
        "    # plt.grid()\n",
        "    # plt.ylabel(\"SmoothL1Loss\")\n",
        "    # plt.plot(loss_history)\n",
        "    # #plt.savefig(\"./ResNet_results/loss_\" +model_name+\".png\")\n",
        "    # plt.show()\n",
        "    \n",
        "    # plt.figure()\n",
        "    # plt.title(\"Loss history zoom - \"+model_name)\n",
        "    # plt.xlim([25, 300])\n",
        "    # plt.xlabel(\"Epochs\")\n",
        "    # plt.grid()\n",
        "    # plt.ylabel(\"SmoothL1Loss\")\n",
        "    # plt.plot(loss_history[25:])\n",
        "    # #plt.savefig(\"./ResNet_results/loss_zoom\" +model_name+\".png\")\n",
        "    # plt.show()\n",
        "    \n",
        "    # plt.figure()\n",
        "    # plt.title(\"Validation loss history - \"+model_name)\n",
        "    # plt.xlabel(\"Epochs\")\n",
        "    # plt.grid()\n",
        "    # plt.ylabel(\"SmoothL1Loss\")\n",
        "    # plt.plot(valid_loss_history)\n",
        "    # #plt.savefig(\"./ResNet_results/val_loss_\" +model_name+\".png\")\n",
        "    # plt.show()\n",
        "\n",
        "    return train_loss.item()\n",
        "\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model, t, title):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(iter(test_dl)):\n",
        "        if torch.cuda.is_available():\n",
        "          inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        # evaluate the model on the test set\n",
        "        yhat = model(inputs.float())\n",
        "        # retrieve numpy array\n",
        "        yhat, targets = yhat.cpu(), targets.cpu()\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        # round to class values\n",
        "        #yhat = yhat.round()\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    y_hat = predictions * (max_y-min_y) + min_y\n",
        "    y = actuals * (max_y-min_y) + min_y\n",
        "    # calculate accuracy\n",
        "    acc = math.sqrt(mean_squared_error(actuals, predictions))\n",
        "    acc_denorm = math.sqrt(mean_squared_error(y, y_hat))\n",
        "    r2 = r2_score(y, y_hat)\n",
        "    \n",
        "    end = time.time()\n",
        "    runtime = end - start\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(np.linspace(25, 300), np.linspace(25, 300), 'magenta')\n",
        "    plt.scatter(y,y_hat, s=5, color='indigo')\n",
        "    plt.xlabel(\"Real\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Real vs Predicted \" + t + \" - \" + title)\n",
        "    plt.grid()\n",
        "    #plt.savefig(\"./ResNet_results/scatter_\" +title+t+\".png\")\n",
        "    plt.show()\n",
        "    \n",
        "    return acc, acc_denorm, r2, runtime, y-y_hat\n",
        "\n",
        " \n",
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "    # convert row to data\n",
        "    row = Tensor([row])\n",
        "    # make prediction\n",
        "    yhat = model(row)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA4lgjRknc1K",
        "outputId": "7a84f411-11a3-4cbb-fb65-3ff4e5a11a3b"
      },
      "source": [
        "import pickle\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "\n",
        "def run_model(hyperparams):\n",
        "\n",
        "    bs=hyperparams[\"bs\"]\n",
        "    lr=hyperparams[\"lr\"]\n",
        "    momentum=hyperparams[\"momentum\"]\n",
        "    optimizer=hyperparams[\"optimizer\"]\n",
        "    model_name = hyperparams[\"model\"]\n",
        "    print(hyperparams)\n",
        "\n",
        "    path = './drive/MyDrive/Tesi/dataset_ratio.csv'\n",
        "    if model_name == \"ResNet18\":\n",
        "      model = ResNet18_Counter()\n",
        "    elif model_name == \"ResNet34\":\n",
        "      model = ResNet34_Counter()\n",
        "\n",
        "    train_dl, valid_dl, test_dl = prepare_data(path,batch_size=bs)\n",
        "    # train the model\n",
        "    train_loss = train_model(train_dl, valid_dl, model, lr, momentum, optimizer, model_name)\n",
        "    print(train_loss)\n",
        "    \n",
        "    return train_loss\n",
        "\n",
        "# trials = Trials()\n",
        "\n",
        "with open('./drive/MyDrive/Tesi/trials.pickle', 'rb') as handle:\n",
        "    trials = pickle.load(handle)\n",
        "\n",
        "space = {\n",
        "    \"bs\": hp.choice(\"bs\", [4,8,16,32,64,128]),\n",
        "    \"lr\": hp.choice(\"lr\", [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1]),\n",
        "    \"momentum\": hp.choice(\"momentum\", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
        "    \"optimizer\": hp.choice(\"optimizer\", [\"adam\",\"sgd\"]),\n",
        "    \"model\": hp.choice(\"model\", [\"ResNet18\", \"ResNet34\"])\n",
        "}\n",
        "\n",
        "best = fmin(\n",
        "    fn=run_model,  \n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    trials=trials,\n",
        "    max_evals=24\n",
        ")\n",
        "\n",
        "with open('./drive/MyDrive/Tesi/trials.pickle', 'wb') as handle:\n",
        "    pickle.dump(trials, handle)\n",
        "print(f\"Optimal value of x: {best}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bs': 8, 'lr': 1e-08, 'model': 'ResNet18', 'momentum': 0.2, 'optimizer': 'adam'}\n",
            "2.632268190383911\n",
            "{'bs': 16, 'lr': 0.001, 'model': 'ResNet34', 'momentum': 0.2, 'optimizer': 'adam'}\n",
            "3.3731873827491654e-07\n",
            "{'bs': 8, 'lr': 1e-10, 'model': 'ResNet18', 'momentum': 0.8, 'optimizer': 'adam'}\n",
            "1.6605051755905151\n",
            "{'bs': 4, 'lr': 1e-07, 'model': 'ResNet18', 'momentum': 0.7, 'optimizer': 'adam'}\n",
            "0.7123553156852722\n",
            "{'bs': 16, 'lr': 1e-08, 'model': 'ResNet34', 'momentum': 0.3, 'optimizer': 'sgd'}\n",
            "0.3848758637905121\n",
            "{'bs': 128, 'lr': 0.1, 'model': 'ResNet18', 'momentum': 0.7, 'optimizer': 'sgd'}\n",
            "nan\n",
            "{'bs': 64, 'lr': 1e-06, 'model': 'ResNet18', 'momentum': 0.5, 'optimizer': 'sgd'}\n",
            "0.1043463796377182\n",
            "{'bs': 128, 'lr': 1e-09, 'model': 'ResNet18', 'momentum': 0.8, 'optimizer': 'adam'}\n",
            " 35%|███▌      | 7/20 [45:12<1:13:54, 341.09s/it, best loss: 3.3731873827491654e-07]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5qN4oUNgTkx"
      },
      "source": [
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = ResNet18_Counter()\n",
        "path = './drive/MyDrive/Tesi/dataset_ratio.csv'\n",
        "model_name = \"ResNet18\"\n",
        "\n",
        "train_dl, valid_dl, test_dl = prepare_data(path,4)\n",
        "print(\"Dataset Prepared\")\n",
        "# train the model\n",
        "train_model(train_dl, valid_dl, model, 1e-3, \"adam\", model_name)\n",
        "# evaluate the model on training\n",
        "acc_train, acc_denorm_train, r2_train, runtime, error_train = evaluate_model(train_dl, model, \"train\", model_name)\n",
        "print(\"\")\n",
        "print('Train RMSE: %.3f' % acc_train)\n",
        "print('Train RMSE (denorm): %.3f' % acc_denorm_train)\n",
        "print('Train R2 score: %.3f' %r2_train)\n",
        "\n",
        "# evaluate the model on validation\n",
        "acc_valid, acc_denorm_valid, r2_valid, runtime, error_valid = evaluate_model(valid_dl, model, \"validation\", model_name)\n",
        "print(\"\")\n",
        "print('Validation RMSE: %.3f' % acc_valid)\n",
        "print('Validation RMSE (denorm): %.3f' % acc_denorm_valid)\n",
        "print('Validation R2 score: %.3f' %r2_valid)\n",
        "\n",
        "# evaluate model on test\n",
        "acc_test, acc_denorm_test, r2_test, runtime_test, error_test = evaluate_model(test_dl, model, \"test\", model_name)\n",
        "print('Test RMSE: %.3f' % acc_test)\n",
        "print('Test RMSE (denorm): %.3f' % acc_denorm_test)\n",
        "print('Test R2 score: %.3f' %r2_test)\n",
        "print(\"Execution time: %.3f\" %runtime)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(error_train, 50, color='dodgerblue')\n",
        "plt.hist(error_test, 50, color='gold')\n",
        "plt.xlabel('Error on estimation')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend([\"Train\",\"Test\"])\n",
        "plt.title('Histogram for the estimation error - ' + model_name)\n",
        "#plt.savefig(\"./ResNet_results/hist_\" +model_name+\".png\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkGftu_jikwC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}